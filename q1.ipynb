{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13257436,"sourceType":"datasetVersion","datasetId":8400884},{"sourceId":597867,"sourceType":"modelInstanceVersion","modelInstanceId":447731,"modelId":464148}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Setup and Installations\n!pip install -q timm pandas\n\nprint(\"Setup complete. Libraries installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:49.971885Z","iopub.execute_input":"2025-10-04T17:49:49.972220Z","iopub.status.idle":"2025-10-04T17:49:53.445758Z","shell.execute_reply.started":"2025-10-04T17:49:49.972192Z","shell.execute_reply":"2025-10-04T17:49:53.444549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Configuration for Fine-Tuning\nfrom dataclasses import dataclass\nimport torch\n\n@dataclass\nclass CFG:\n    # --- Data ---\n    dataset: str = \"CIFAR10\"\n    # Set your data root. For Kaggle, it's usually \"/kaggle/input/cifar-10/cifar-10-batches-py\"\n    # For Colab, you might need to download it first.\n    data_root: str = \"/kaggle/input/cifar-10/cifar-10-batches-py\"\n    num_classes: int = 10\n    \n    # --- Fine-Tuning Resolution ---\n    # We increase the image size for fine-tuning. Your model was trained on 48x48.\n    # Let's fine-tune on 64x64. This forces the model to learn finer details.\n    img_size: int = 64\n    \n    # --- Pre-trained Model Path ---\n    # Path to your best model file.\n    pretrained_path: str = \"/kaggle/input/cifar_vit_v0/pytorch/default/1/best_vit_cifar10.pth\"\n    # Original image size the model was trained on. This is CRITICAL for position embedding interpolation.\n    original_img_size: int = 48\n\n    # --- Model Architecture (should match your pre-trained model) ---\n    patch_size: int = 4\n    embed_dim: int = 512\n    depth: int = 8\n    num_heads: int = 8\n    mlp_ratio: float = 4.0\n    qkv_bias: bool = True\n    drop_rate: float = 0.1\n    attn_drop_rate: float = 0.0\n    drop_path_rate: float = 0.2\n\n    # --- Fine-Tuning Schedule ---\n    # Fine-tuning requires fewer epochs and a smaller learning rate.\n    epochs: int = 50\n    batch_size: int = 128\n    optimizer: str = \"adamw\" # Paper suggests SGD with momentum for fine-tuning \n    base_lr: float = 5e-4  # Smaller LR for fine-tuning\n    min_lr: float = 1e-6\n    weight_decay: float = 0.05\n    warmup_epochs: int = 5\n    mixup_freeze_epochs: int = 10  # disable MixUp/CutMix/LS for first N epochs\n\n\n    # --- Augmentations & Tricks ---\n    # We can slightly reduce strong regularization during fine-tuning.\n    label_smoothing: float = 0.1\n    mixup_alpha: float = 0.2\n    cutmix_alpha: float = 1.0\n    random_erasing_p: float = 0.1\n\n    # --- Training Niceties ---\n    amp: bool = True\n    compile: bool = True\n    ema_decay: float = 0.9999 # Slightly stronger EMA\n    grad_accum_steps: int = 1\n    out_dir: str = \"./outputs\"\n    seed: int = 42\n\ncfg = CFG()\n\n# Set device and workers\ncfg.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncfg.num_workers = 2 # Or 4 if your machine supports it","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:53.447685Z","iopub.execute_input":"2025-10-04T17:49:53.447960Z","iopub.status.idle":"2025-10-04T17:49:53.459781Z","shell.execute_reply.started":"2025-10-04T17:49:53.447934Z","shell.execute_reply":"2025-10-04T17:49:53.459011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Imports\nimport os\nimport time\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\n\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.data.mixup import Mixup\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.scheduler.cosine_lr import CosineLRScheduler\n\n# For EMA\nfrom copy import deepcopy\n\n# For plots\nimport matplotlib.pyplot as plt\n\nprint(\"Imports complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:53.460771Z","iopub.execute_input":"2025-10-04T17:49:53.461333Z","iopub.status.idle":"2025-10-04T17:49:53.480986Z","shell.execute_reply.started":"2025-10-04T17:49:53.461308Z","shell.execute_reply":"2025-10-04T17:49:53.480254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Utilities\n\ndef seed_everything(seed: int):\n    \"\"\"Set random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass ModelEmaV2(nn.Module):\n    \"\"\" Model Exponential Moving Average V2\n    From timm library.\n    \"\"\"\n    def __init__(self, model, decay=0.9999, device=None):\n        super(ModelEmaV2, self).__init__()\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n        self.device = device\n        if self.device is not None:\n            self.module.to(device=device)\n\n    def _update(self, model, update_fn):\n        with torch.no_grad():\n            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n                if self.device is not None:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)\n\nseed_everything(cfg.seed)\nprint(\"Utilities defined and seed set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:53.483176Z","iopub.execute_input":"2025-10-04T17:49:53.483408Z","iopub.status.idle":"2025-10-04T17:49:53.499220Z","shell.execute_reply.started":"2025-10-04T17:49:53.483392Z","shell.execute_reply":"2025-10-04T17:49:53.498601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Data Loading and Augmentations\n\ndef get_datasets(img_size):\n    # Mean and std for CIFAR-10\n    mean, std = (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)\n\n    # Strong augmentations for training\n    train_transform = transforms.Compose([\n        transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC),\n        transforms.RandomCrop(img_size, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.TrivialAugmentWide(), # A good auto-augmentation policy\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n        transforms.RandomErasing(p=cfg.random_erasing_p, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n    ])\n\n    # Simple transforms for validation\n    val_transform = transforms.Compose([\n        transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n\n    train_dataset = datasets.CIFAR10(\n        root=\"/kaggle/working/\",\n        train=True,\n        download=True,       #  allow download now\n        transform=train_transform\n    )\n    val_dataset = datasets.CIFAR10(\n        root=\"/kaggle/working/\",\n        train=False,\n        download=True,\n        transform=val_transform\n    )\n    \n    return train_dataset, val_dataset\n\ntrain_dataset, val_dataset = get_datasets(cfg.img_size)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n\nprint(f\"Data loaded. Training on {len(train_dataset)} images, validating on {len(val_dataset)} images.\")\nprint(f\"Image size for fine-tuning: {cfg.img_size}x{cfg.img_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:53.499879Z","iopub.execute_input":"2025-10-04T17:49:53.500048Z","iopub.status.idle":"2025-10-04T17:49:55.247967Z","shell.execute_reply.started":"2025-10-04T17:49:53.500034Z","shell.execute_reply":"2025-10-04T17:49:55.247097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Vision Transformer (ViT) Model Definition\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        self.blocks = nn.ModuleList([\n            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return self.head(x[:, 0])\n\ndef interpolate_pos_encoding(model, checkpoint_model):\n    # Try to find the pos_embed key automatically\n    pos_keys = [k for k in checkpoint_model.keys() if \"pos_embed\" in k]\n    if not pos_keys:\n        raise KeyError(\"No 'pos_embed' key found in checkpoint!\")\n    previous_pos_embed = checkpoint_model[pos_keys[0]]\n\n    current_pos_embed = model.pos_embed\n    num_patches_new = current_pos_embed.shape[1] - 1\n    num_patches_old = previous_pos_embed.shape[1] - 1\n\n    if num_patches_new == num_patches_old:\n        print(\" Positional embeddings match. No interpolation needed.\")\n        return previous_pos_embed\n\n    print(f\"Interpolating position embeddings from {num_patches_old} â†’ {num_patches_new} patches\")\n\n    cls_pos_embed = previous_pos_embed[:, 0]\n    patch_pos_embed = previous_pos_embed[:, 1:]\n    dim = patch_pos_embed.shape[-1]\n\n    old_size = int(math.sqrt(num_patches_old))\n    new_size = int(math.sqrt(num_patches_new))\n    patch_pos_embed = patch_pos_embed.reshape(1, old_size, old_size, dim).permute(0, 3, 1, 2)\n    patch_pos_embed = F.interpolate(patch_pos_embed, size=(new_size, new_size), mode='bicubic', align_corners=False)\n    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).reshape(1, -1, dim)\n\n    new_pos_embed = torch.cat((cls_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n    return new_pos_embed\n\n\nprint(\"ViT model definition complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:55.249051Z","iopub.execute_input":"2025-10-04T17:49:55.249786Z","iopub.status.idle":"2025-10-04T17:49:55.277574Z","shell.execute_reply.started":"2025-10-04T17:49:55.249763Z","shell.execute_reply":"2025-10-04T17:49:55.276774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Model Initialization and Weight Loading\n\n# 1. Instantiate model\nmodel = VisionTransformer(\n    img_size=cfg.img_size,\n    patch_size=cfg.patch_size,\n    num_classes=cfg.num_classes,\n    embed_dim=cfg.embed_dim,\n    depth=cfg.depth,\n    num_heads=cfg.num_heads,\n    mlp_ratio=cfg.mlp_ratio,\n    qkv_bias=cfg.qkv_bias,\n    drop_rate=cfg.drop_rate,\n    attn_drop_rate=cfg.attn_drop_rate,\n    drop_path_rate=cfg.drop_path_rate\n).to(cfg.device)\n\n# 2. Load pretrained weights\nif os.path.exists(cfg.pretrained_path):\n    print(f\"Loading pre-trained weights from {cfg.pretrained_path}\")\n    checkpoint = torch.load(cfg.pretrained_path, map_location=cfg.device)\n\n    # unwrap model or model_ema\n    checkpoint_model = checkpoint.get(\"model\") or checkpoint.get(\"model_ema\") or checkpoint\n\n    # strip 'module.' prefixes\n    state_dict = OrderedDict()\n    for k, v in checkpoint_model.items():\n        if k.startswith(\"module.\"):\n            name = k[len(\"module.\"):]\n        elif k.startswith(\"_orig_mod.\"):\n            name = k[len(\"_orig_mod.\"):]\n        else:\n            name = k\n        state_dict[name] = v\n\n\n    # --- a. interpolate positional embeddings safely ---\n    pos_keys = [k for k in state_dict.keys() if \"pos_embed\" in k]\n    if not pos_keys:\n        raise KeyError(\"No 'pos_embed' key found in checkpoint!\")\n    pos_key = pos_keys[0]\n    state_dict[pos_key] = interpolate_pos_encoding(model, {\"pos_embed\": state_dict[pos_key]})\n\n    # --- b. remove old classification head (robustly) ---\n    for key in [\"head.weight\", \"head.bias\", \"fc.weight\", \"fc.bias\", \"classifier.weight\", \"classifier.bias\"]:\n        if key in state_dict:\n            del state_dict[key]\n            print(f\"Removed {key} from checkpoint.\")\n\n    # --- c. load state dict with strict=False ---\n    msg = model.load_state_dict(state_dict, strict=False)\n    print(f\"Weight loading message: {msg}\")\n\n    # --- d. re-init final head ---\n    print(\"Re-initializing classification head.\")\n    model.head.weight.data.mul_(0.001)\n    model.head.bias.data.zero_()\nelse:\n    print(f\"No checkpoint found at {cfg.pretrained_path}. Training from scratch.\")\n\n# (Optional) Compile\nif cfg.compile and hasattr(torch, \"compile\"):\n    print(\"Compiling model...\")\n    model = torch.compile(model)\n\nprint(\"Model ready for fine-tuning.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:55.278559Z","iopub.execute_input":"2025-10-04T17:49:55.278862Z","iopub.status.idle":"2025-10-04T17:49:55.809483Z","shell.execute_reply.started":"2025-10-04T17:49:55.278838Z","shell.execute_reply":"2025-10-04T17:49:55.808718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Loss, Optimizer, and Scheduler\n\n# Mixup function for data augmentation\nmixup_fn = Mixup(\n    mixup_alpha=cfg.mixup_alpha,\n    cutmix_alpha=cfg.cutmix_alpha,\n    label_smoothing=cfg.label_smoothing,\n    num_classes=cfg.num_classes\n)\n\n# Loss function\nif cfg.mixup_alpha > 0.:\n    criterion = SoftTargetCrossEntropy()\nelif cfg.label_smoothing > 0.:\n    criterion = LabelSmoothingCrossEntropy(smoothing=cfg.label_smoothing)\nelse:\n    criterion = nn.CrossEntropyLoss()\n\n# Optimizer\nif cfg.optimizer.lower() == 'sgd':\n    optimizer = torch.optim.SGD(model.parameters(), lr=cfg.base_lr, momentum=0.9, weight_decay=cfg.weight_decay)\nelse: # Default to AdamW\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.base_lr, weight_decay=cfg.weight_decay)\n\n# Learning Rate Scheduler\nlr_scheduler = CosineLRScheduler(\n    optimizer,\n    t_initial=cfg.epochs,\n    lr_min=cfg.min_lr,\n    warmup_t=cfg.warmup_epochs,\n    warmup_lr_init=1e-6,\n    warmup_prefix=True\n)\n\nprint(f\"Using criterion: {type(criterion).__name__}\")\nprint(f\"Using optimizer: {type(optimizer).__name__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:55.810335Z","iopub.execute_input":"2025-10-04T17:49:55.810665Z","iopub.status.idle":"2025-10-04T17:49:55.818238Z","shell.execute_reply.started":"2025-10-04T17:49:55.810635Z","shell.execute_reply":"2025-10-04T17:49:55.817564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Training and Validation Functions\n\ndef train_one_epoch(model, loader, optimizer, criterion, device, epoch, scaler, mixup_fn, model_ema):\n    model.train()\n    total_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    start_time = time.time()\n    \n    for i, (inputs, targets) in enumerate(loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        # Apply mixup\n        # MixUp/CutMix warmup: disable for first cfg.mixup_freeze_epochs\n        if epoch < cfg.mixup_freeze_epochs:\n            samples = inputs\n            targets = torch.nn.functional.one_hot(targets, num_classes=cfg.num_classes).float()\n        else:\n            samples, targets = mixup_fn(inputs, targets)\n\n        \n        with torch.cuda.amp.autocast(enabled=cfg.amp):\n            outputs = model(samples)\n            loss = criterion(outputs, targets)\n        \n        scaler.scale(loss).backward()\n        # Grad clipping (after unscale, before step)\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        \n        if model_ema is not None:\n            model_ema.update(model)\n\n        total_loss += loss.item()\n        \n        # Accuracy for logging\n        _, predicted = outputs.max(1)\n        _, true_labels = targets.max(1) # Un-mix labels for accuracy\n        total_correct += predicted.eq(true_labels).sum().item()\n        total_samples += targets.size(0)\n\n        if i % 50 == 0:\n            print(f\"  Batch {i}/{len(loader)} | Loss: {loss.item():.4f}\")\n\n    epoch_time = time.time() - start_time\n    avg_loss = total_loss / len(loader)\n    accuracy = 100. * total_correct / total_samples\n    \n    print(f\"End of Epoch {epoch+1} | Train Time: {epoch_time:.2f}s | Avg Loss: {avg_loss:.4f} | Train Acc: {accuracy:.2f}%\")\n    return avg_loss, accuracy\n\n\n@torch.no_grad()\ndef validate(model, loader, device):\n    model.eval()\n    total_correct = 0\n    total_samples = 0\n    \n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        with torch.cuda.amp.autocast(enabled=cfg.amp):\n            logits = model(inputs)\n            logits_flip = model(torch.flip(inputs, dims=[-1]))\n            outputs = (logits + logits_flip) / 2\n\n        _, predicted = outputs.max(1)\n        total_correct += predicted.eq(targets).sum().item()\n        total_samples += targets.size(0)\n        \n    accuracy = 100. * total_correct / total_samples\n    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n    return accuracy\n\nprint(\"Training and validation functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:55.819255Z","iopub.execute_input":"2025-10-04T17:49:55.819485Z","iopub.status.idle":"2025-10-04T17:49:55.857880Z","shell.execute_reply.started":"2025-10-04T17:49:55.819470Z","shell.execute_reply":"2025-10-04T17:49:55.857322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Main Fine-Tuning Loop\n\nprint(\"Starting fine-tuning...\")\n\n# Create output directory\nos.makedirs(cfg.out_dir, exist_ok=True)\n\n# Setup scaler for AMP\nscaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n\n# Setup EMA\nmodel_ema = ModelEmaV2(model, decay=cfg.ema_decay, device=cfg.device)\n\nbest_acc = 0.0\nhistory = []\n\nstart_total_time = time.time()\n\nfor epoch in range(cfg.epochs):\n    print(\"-\" * 50)\n    current_lr = optimizer.param_groups[0][\"lr\"]\n    print(f\"Epoch {epoch+1}/{cfg.epochs} | LR: {current_lr:.6f}\")\n    \n    # Train\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, cfg.device, epoch, scaler, mixup_fn, model_ema)\n    \n    # Update LR scheduler\n    lr_scheduler.step(epoch + 1)\n    \n    # Validate with EMA model for stability\n    val_acc = validate(model_ema.module, val_loader, cfg.device)\n    \n    history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': train_acc, 'val_acc': val_acc, 'lr': current_lr})\n    \n    if val_acc > best_acc:\n        best_acc = val_acc\n        print(f\"New best accuracy: {best_acc:.2f}%! Saving model...\")\n        save_path = os.path.join(cfg.out_dir, \"best_finetuned_vit.pth\")\n        torch.save({\n            'model_ema': model_ema.module.state_dict(),\n            'config': cfg,\n            'epoch': epoch,\n        }, save_path)\n\ntotal_training_time = time.time() - start_total_time\nprint(f\"\\nFine-tuning finished in {total_training_time/60:.2f} minutes.\")\nprint(f\"Best validation accuracy: {best_acc:.2f}%\")\n\n# Create a DataFrame for easy viewing\nhistory_df = pd.DataFrame(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T17:49:55.859781Z","iopub.execute_input":"2025-10-04T17:49:55.859997Z","iopub.status.idle":"2025-10-04T21:02:08.106309Z","shell.execute_reply.started":"2025-10-04T17:49:55.859981Z","shell.execute_reply":"2025-10-04T21:02:08.105411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: Results Visualization and README Generation\n\n# Plotting the results\nplt.figure(figsize=(12, 5))\n\n# Plot Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history_df['epoch'], history_df['train_acc'], label='Train Accuracy')\nplt.plot(history_df['epoch'], history_df['val_acc'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\nplt.grid(True)\n\n# Plot Loss\nplt.subplot(1, 2, 2)\nplt.plot(history_df['epoch'], history_df['train_loss'], label='Train Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T21:02:08.107562Z","iopub.execute_input":"2025-10-04T21:02:08.107836Z","iopub.status.idle":"2025-10-04T21:02:08.791299Z","shell.execute_reply.started":"2025-10-04T21:02:08.107807Z","shell.execute_reply":"2025-10-04T21:02:08.790503Z"}},"outputs":[],"execution_count":null}]}